{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Computer Architecture for Data Scientists: from the transistor to The Cloud\n",
    "\n",
    "### Kevin Moore\n",
    "** Data Science BIG, April 11 2017 **\n",
    "\n",
    "It's an amazing time to be a data scientist or a computer-savvy scientist. My laptop is like a supercomputer from 20 years ago. And, more computing power than most of us can imagine is available to all of us in the public cloud.\n",
    "\n",
    "About Me:\n",
    "* CEO Quilt Data\n",
    "* PhD Computer Architecture\n",
    "* MP memory systems & DBMS \n",
    "* Oracle and Sun Labs\n",
    "* **not a biologist**\n",
    "\n",
    "** Goal today: **\n",
    "Explain a little about how computers work and what makes them go fast. Understanding that will (hopefully) help you:\n",
    "* write better software\n",
    "* make more informed software choices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Example: intersect genomic regions\n",
    "\n",
    "Let's say for example that we want to take a published CRISPR library from the well-known Lander-Sabatini Lab and intersect it with the set of common genetic variants known to be associated with a medial condition (published as part of the CLINVAR dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Why is this so much faster?\n",
    "def intersect_fast(a, b):\n",
    "    matches = None\n",
    "\n",
    "    a_chrs = a.groupby('Chromosome')\n",
    "    b_chrs = b.groupby('Chromosome')\n",
    "    \n",
    "    for a_chr, a_grp in a_chrs:\n",
    "        b_grp = b_chrs.get_group(a_row['Chromosome'])\n",
    "        for idx, a_row in a_grp.iterrows():\n",
    "            a_match = b_grp.loc[(b_grp['start'] < a_row['end']) & ((b_grp['end'] > a_row['start']))]\n",
    "            matches = pd.concat([matches, a_match]) if matches is not None else a_match\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# than this?\n",
    "def intersect_naive(a, b):\n",
    "    matches = []\n",
    "    for a in rowmajor_a:\n",
    "        for b in rowmajor_b:\n",
    "            if a[0] == b[0] and int(a[1]) < int(b[2]) and int(a[2]) > int(b[1]):\n",
    "                matches.append((a, b))\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The code in these two examples perform the same genomic intersection. Both are implemented in Python and neither uses explicit multiprocessing. But, the second example runs more than 100x faster. I'll use these loops and problem to illustrate some of the central concepts and techniques in computer architecture and explain some techniques for writing fast code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Key Concepts in Computer Architecture: Latency and Bandwidth\n",
    "### Latency\n",
    "* how long a single unit of a resource takes to arrive\n",
    "* affected by size and distance of components\n",
    "* changes slowly with technology generation (like clock rate)  \n",
    "\n",
    "### Bandwidth\n",
    "* how much of a resource can be delivered per unit time\n",
    "* can often 'buy' more with money/cost, power, area, etc.\n",
    "* gets cheaper with tech\n",
    "\n",
    "The architect's goal is to be limited by bandwidth and \"buy\" only the bandwidth needed for the given system. To get there, we try to **hide latency** (by doing something else instead of waiting)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Computer Architect’s secret weapons\n",
    "### Locality\n",
    "Architects take advantage of locality to **reduce latency** by:\n",
    "* using smaller structures (e.g. caches instead of memory)\n",
    "* making fewer, larger lookups (e.g. on disk and in memory)\n",
    "\n",
    "### Speculation    \n",
    "Speculation is making a prediction with the ability to undo any actions if the prediction turns out to be wrong. Speculation allows architects to **hide latency** by designing processors to do useful work while waiting for long-latency results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Von Neumann Model (1945)\n",
    "In many ways computers haven’t changed much since the late 1940’s. Jon Von Neumann is credited with introducing the stored program computer in 1945. It had the following basic set of components:\n",
    "\n",
    "* Memory\n",
    "* Memory Address Register (MAR)\n",
    "* Memory Data Register (MDR)\n",
    "* ALU/TEMP(registers)\n",
    "* Control\n",
    "* I/O\n",
    "\n",
    "That first computer and computers today execute instructions by following the series of basic steps:\n",
    "![title](instruction.jpg)\n",
    "\n",
    "Since then, switches are much smaller and faster (now transistors on ICs). Computers got so much smaller, faster and cheaper they became common in offices and homes in the 1980s. Even since then, computers have gotten much, much, faster. For example:\n",
    "\n",
    "| Intel 386 (1989) | MacbookPro/Intel Skylake (2016)|\n",
    "| ------------ | :-----------: |\n",
    "| 33 MHz  | 2.9 GHz |\n",
    "| 275K transistors | more than 1B Transistors |\n",
    "| 4.3 MIPS  | 317 GIPS ||\n",
    "\n",
    "**74000x faster**\n",
    "~100x faster clock  \n",
    "~740x improved architecture (**parallelism**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## But my macbook only has 4 cores, where does that parallelism come from?\n",
    "### Instruction-Level Parallelism\n",
    "\n",
    "You might think that most of that speedup comes from the transistors getting faster, but the clock rate is only 100-times faster. Most of the speedup--the other 740x comes from **parallelism**. My laptop is a 4-core so we can chalk up 4x speedup to parallism. But, that still leaves 185x speedup from internal parallelism enabled by improved computer architecture and lots more transistors.\n",
    "\n",
    "* **Pipelining**\n",
    "    + ~5x parallelism\n",
    "    + example below\n",
    "\n",
    "* **Superscalar**\n",
    "    + ~2x more parallelism\n",
    "    + Complicated, very cool, next lecture\n",
    "\n",
    "* **SIMD/Vector Instructions**\n",
    "    + 16x+ parallelism\n",
    "    + only for very regular execution patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Pipelined Execution\n",
    "\n",
    "Each instruction takes multiple (5 or more in the example below), but multiple instructions are executing at the same time. The example below shows a simple processor pipeline with 5 stages. Instructions move right to left as they execute various stages.\n",
    "\n",
    "![title](pipeline_stages.jpg)\n",
    "\n",
    "Modern Intel processors have more complicated pipelines that are both **super-scalar** (fetch and execute multiple instructions per cycle) and **out-of-order** (the processor executes instructions as soon as they are ready possibly bypassing earlier waiting instructions).\n",
    "\n",
    "But, to make that work procs need:\n",
    "* **Caches**  \n",
    "* **Branch prediction** (guessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Caches \n",
    "\n",
    "Computer memory system is a hierarchy:\n",
    "* Registers (many accesses per cycle)\n",
    "* L1 Cache - usually 1 cycle (10s of KBs)\n",
    "* L2 Cache - usually 2-3 cycles (100s of KBs)\n",
    "* L3 Cache - 10s of cycles (MBs)\n",
    "* Main memory - 100s of cycles (GBs)\n",
    "* Virtual memory (SSD) - 1000s of cycles (100s of GBs)\n",
    "\n",
    "Caches store a small subset of memory that is loaded on demand. When a processor loads a memory location, it looks in its closest/smallest caches first. If the memory address isn't cached, it issues a request to higher levels of memory system. When the data are returned, they're loaded into the cache with the expectation that they're likely to be accessed again. Most software has a small working set in memory that is accessed more frequently than other locations. Caches (ideally) hold that working set for the duration of a program and speedup most memory accesses.\n",
    "\n",
    "Caches are designed to take advantage of two forms of locality: **temporal locality** (the same memory location is accessed repeatedly in a short period of time) and **spatial locality** (adjacent memory locations are accessed together)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Branch Prediction\n",
    "\n",
    "Instructions that change the flow of instructions (e.g. to re-execute the body of a loop) are called branches. To avoid stalling the processor's pipeline, the instructions following a branch need to be fetched before the branch itself is executed.\n",
    "\n",
    "![title](pipeline-branch-pred.jpg)\n",
    "\n",
    "Processors speculate (guess) whether or not the branch will be taken, and flush the pipeline if they're wrong. Flushes are only a little more expensive than not speculating and can help bring instructions and data into processor caches faster. From a software developer's perspective, our goal is to write software with control flow that's easy to guess (e.g., **short tight loops with little internal branching**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Back to biology…\n",
    "\n",
    "Why is this so slow?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def intersect_naive(rowmajor_a, rowmajor_b):\n",
    "    matches = []\n",
    "    for a in rowmajor_a:\n",
    "        for b in rowmajor_b:\n",
    "            if a[0] == b[0] and int(a[1]) < int(b[2]) and int(a[2]) > int(b[1]):\n",
    "                matches.append((a, b))\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### First some code to setup our experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import signal\n",
    "import time\n",
    "\n",
    "# Datasets used in the examples\n",
    "# see https://quiltdata.com for more info\n",
    "# about Quilt.\n",
    "from quilt.data.kmoore import CAforDS\n",
    "\n",
    "def handler(signum, frame):\n",
    "    print('Stopped')\n",
    "    signal.alarm(0)\n",
    "    raise TimerException(\"Alarm\")\n",
    "    \n",
    "class TimerException(Exception):\n",
    "    pass\n",
    "\n",
    "# Cut the examples off if they haven't finished\n",
    "# at the end of the timer so you don't have to\n",
    "# wait all day.\n",
    "signal.signal(signal.SIGALRM, handler)\n",
    "timer = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# CRISPR Library from Lander-Sabatini Lab:\n",
    "# - gRNA Name\n",
    "# - Gene\n",
    "# - Chromosome\n",
    "# - Start\n",
    "# - Stop\n",
    "# - Strand\n",
    "# - Sequence\n",
    "# - Sequence revComp\n",
    "# - oligo_F\n",
    "# - oligo_R\n",
    "# - sgA1BG_1\n",
    "\n",
    "with open(CAforDS.raw.lander(), 'r') as a_bed:\n",
    "    bedreader = csv.reader(a_bed, delimiter='\\t')\n",
    "    rowmajor_a = [row for row in bedreader]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Clinvar Common-and-Known\n",
    "# - Chromosome\n",
    "# - Position\n",
    "# - RSID\n",
    "# - Ref\n",
    "# - Alt\n",
    "# - Info\n",
    "with open(CAforDS.raw.clinvar(), 'r') as b_bed:\n",
    "    bedreader = csv.reader(b_bed, delimiter='\\t')\n",
    "    rowmajor_b = [row for row in bedreader if not row[0].startswith('#')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Naive Implementation: Nested Loops over rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped\n",
      "Excecuted 20s\n",
      "Completed 24.811452M iterations (2.36%)\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "a_len = len(rowmajor_a)\n",
    "b_len = len(rowmajor_b)\n",
    "outrows = []\n",
    "signal.alarm(timer)\n",
    "try:\n",
    "    for a in rowmajor_a[1:]:\n",
    "        # Read a row from A\n",
    "        a_chr = a[2]\n",
    "        a_start = int(a[3])\n",
    "        a_end = int(a[4])\n",
    "        for b in rowmajor_b[1:]:\n",
    "            # Read a row from B\n",
    "            b_chr = \"chr%s\" % b[0]\n",
    "            b_pos = int(b[1])\n",
    "            if a_chr == b_chr and a_start <= b_pos and b_pos <= a_end:\n",
    "                outrows.append(b)\n",
    "            count += 1\n",
    "    print(\"Finished\")\n",
    "except TimerException:\n",
    "    print(\"Excecuted %ss\" % timer)\n",
    "finally:\n",
    "    signal.alarm(0)\n",
    "    miterations = count/1000000.0\n",
    "    pct = count/(a_len*b_len)*100.0\n",
    "    print(\"Completed %sM iterations (%0.2f%%)\" % (miterations, pct))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Why is this so slow?\n",
    "\n",
    "On my laptop, I get about 2.5% through the calculation in 20s so the whole thing would take around 14 minutes when the fast version will execute in about 5s.\n",
    "\n",
    "Problem 1: **Algorithm**\n",
    "\n",
    "An all-to-all comparison requires A * B operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181131 x 5807 = 1051827717 (1.05B)\n"
     ]
    }
   ],
   "source": [
    "acount = len(rowmajor_a)\n",
    "bcount = len(rowmajor_b)\n",
    "print(\"%d x %d = %d (%.2fB)\" % (acount, bcount, acount*bcount, (float(acount)*bcount/(10**9))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Smarter approach: Partition by chromosome\n",
    "\n",
    "Only genomic regions on the same Chromosome could possibly overlap, so we can dramatically reduce the number of comparisons by partitioning the inputs by chromosome and only comparing each region to the subset in the opposite input that matches its chromosome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53839387 (53M) operations\n"
     ]
    }
   ],
   "source": [
    "cc_a = {}\n",
    "cc_b = {}\n",
    "# Count B by chromosome\n",
    "for bidx, b in enumerate(rowmajor_b[1:]):\n",
    "    chrm = \"chr%s\" % b[0]\n",
    "    if chrm not in cc_b:\n",
    "        cc_b[chrm] = 0\n",
    "    cc_b[chrm] += 1 \n",
    "    \n",
    "# Count A by chromosome\n",
    "for aidx, a in enumerate(rowmajor_a[1:]):\n",
    "    chrm = a[2]\n",
    "    if chrm not in cc_a:\n",
    "        cc_a[chrm] = 0\n",
    "    cc_a[chrm] += 1 \n",
    "\n",
    "#print(cc_a.keys())\n",
    "#print(cc_b.keys())\n",
    "\n",
    "total = 0\n",
    "for chrm, count in cc_b.items():\n",
    "    total += count*cc_a[chrm] if chrm in cc_a else 0\n",
    "print(\"%d (%dM) operations\" % (total, total/1000000.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Reduces the work needed from 1B operations to 53M (**20x**). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped\n",
      "Excecuted 20s\n",
      "Completed 20.872505M iterations (38.77%)\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "outrows = []\n",
    "signal.alarm(timer)\n",
    "parts = {}\n",
    "try:\n",
    "    # Partition B\n",
    "    starttime = time.time()\n",
    "    for bidx, b in enumerate(rowmajor_b[1:]):\n",
    "        chrm = \"chr%s\" % b[0]\n",
    "        if chrm not in parts:\n",
    "            parts[chrm] = []\n",
    "        parts[chrm].append(bidx)\n",
    "    \n",
    "    # Iterate through A\n",
    "    for a in rowmajor_a[1:]:\n",
    "        a_chr = a[2]\n",
    "        a_start = int(a[3])\n",
    "        a_end = int(a[4])\n",
    "        if a_chr in parts:\n",
    "            for bidx in parts[a_chr]: \n",
    "                b = rowmajor_b[bidx+1]\n",
    "                b_chr = \"chr%s\" % b[0]\n",
    "                b_pos = int(b[1])\n",
    "                if a_start <= b_pos and b_pos <= a_end:\n",
    "                    outrows.append((a, b))\n",
    "                count += 1            \n",
    "    endtime = time.time()\n",
    "    matches = len(outrows)\n",
    "    print(\"Done, found %s matches in %ds\" % (matches, endtime-starttime))\n",
    "\n",
    "except TimerException:\n",
    "    print(\"Excecuted %ss\" % timer)\n",
    "finally:\n",
    "    signal.alarm(0)\n",
    "    miterations = count/1000000.0\n",
    "    pct = count/total*100.0\n",
    "    print(\"Completed {it}M iterations ({pct:.2f}%)\".format(it=miterations, pct=pct))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "That's a lot faster, but not quite 20x faster because the partitioning takes time.\n",
    "\n",
    "But, it's still **slow**. What else is going wrong?\n",
    "\n",
    "Problem 2: **Interpretation**\n",
    "Python is an interpreted language so even basic operations (e.g. adding a pair of numbers) becomes a much longer set of steps. But, Python has hooks for developers to call out to functions written in C, which is compiled into machine-native binary code. That lets them publish Python libraries where the inner loops run at full speed for most of the calculation.\n",
    "\n",
    "Problem 3: **Inefficent use of caches**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### What happens to caches?\n",
    "\n",
    "If we read genomic interval data into memory in the way it's stored in a BED file, each genomic interval record (chromosome, start, end, strand, score, etc.) will be stored contiguously. Running the code above, the processor will load its caches 64-bytes at a line. As a result, the caches fill up with lots of unused data (score, name, block starts, etc.).\n",
    "\n",
    "![title](cache_pollution.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Branches are hard to predict\n",
    "\n",
    "Inside the processor, comparing strings is actually more complicated than it looks. The processor has to test if each string is ending after each character as well as testing if the characters from the inputs match. And, mixing the chromosome comparison with a comparison of start and stop only adds to the number of branches the processor has to predict on each iteration of the loop. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Alternative: columnar processing\n",
    "\n",
    "Instead, we can represent the set of genomic intervals as a collection of arrays:\n",
    "* Chromosome []\n",
    "* Start []\n",
    "* End []\n",
    "* ...\n",
    "\n",
    "![title](cache_columns.jpg)\n",
    "\n",
    "Accessing the data as columnar arrays leads to:\n",
    "* Better cache locality (only bring in what we need)\n",
    "* Less latency (fewer misses)\n",
    "* More efficient use of BW (only loading what we need)\n",
    "* Tighter loops (less branching)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Pandas\n",
    "====\n",
    "\n",
    "Pandas is a popular and very useful data science package for Python. It stores and operates on data in columnar form. It's built on top of numpy, a package of optimized array operations for fast numeric processing in Python. Using Pandas is an easy to way to avoid interpretation overhead in Python and use columnar processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "lander = CAforDS.df.lander()\n",
    "clinvar = pd.read_csv(CAforDS.raw.clinvar(),\n",
    "                      sep='\\t',\n",
    "                      comment='#',\n",
    "                      names=['CHROM', 'POS', 'ID', 'REF', 'ALT', 'QUAL', 'FILTER', 'INFO'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "a = clinvar\n",
    "b = lander\n",
    "\n",
    "# match chromosome notation (chr<N>)\n",
    "a['Chromosome'] = a['CHROM'].apply(lambda x: \"chr%s\" %x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['CHROM', 'POS', 'ID', 'REF', 'ALT', 'QUAL', 'FILTER', 'INFO',\n",
      "       'Chromosome'],\n",
      "      dtype='object')\n",
      "Index(['gRNAname', 'gene', 'chr', 'start', 'stop', 'strand', 'sequence',\n",
      "       'sequence_revComp', 'oligo_F', 'oligo_R'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(a.columns)\n",
    "print(b.columns)\n",
    "a_len = len(a.index)\n",
    "b_len = len(b.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped\n",
      "Excecuted 20s\n",
      "Completed 210.65M iterations (20.03%)\n"
     ]
    }
   ],
   "source": [
    "matches = []\n",
    "count = 0\n",
    "signal.alarm(timer)\n",
    "try:\n",
    "    starttime = time.time()\n",
    "    for idx, a_row in a.iterrows():\n",
    "        a_match = b.loc[(b['chr'] == a_row['Chromosome']) & (b['start'] <= a_row['POS'])\n",
    "                        & ((a_row['POS'] <= b['stop']))]\n",
    "        matches.append(a_match)\n",
    "        count += b_len\n",
    "    allmatches = pd.concat(matches)\n",
    "    endtime = time.time()\n",
    "    print(\"Done, found %s matches in %ds\" % (len(allmatches), endtime-starttime))\n",
    "except TimerException:\n",
    "    print(\"Excecuted %ss\" % timer)\n",
    "finally:\n",
    "    signal.alarm(0)\n",
    "    miterations = count/1000000.0\n",
    "    pct = count/(a_len*b_len)*100.0\n",
    "    print(\"Completed %.2fM iterations (%0.2f%%)\" % (miterations, pct))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "That's a lot faster, but not enough to make up for a slow algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Partition Large Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done, found 462 matches in 17s\n",
      "Completed 53.86M iterations (100.03%)\n"
     ]
    }
   ],
   "source": [
    "matches = []\n",
    "count = 0\n",
    "signal.alarm(timer)\n",
    "try:\n",
    "    starttime = time.time()\n",
    "    b_chr = b.groupby('chr')\n",
    "    for idx, a_row in a.iterrows():\n",
    "        chr_grp = b_chr.get_group(a_row['Chromosome'])\n",
    "        a_match = chr_grp.loc[(chr_grp['start'] <= a_row['POS'])\n",
    "                              & ((a_row['POS'] <= chr_grp['stop']))]\n",
    "        if len(a_match.index) > 0:\n",
    "            matches.append(a_match)\n",
    "        count += len(chr_grp)\n",
    "    allmatches = pd.concat(matches)\n",
    "    endtime = time.time()\n",
    "    print(\"Done, found %s matches in %ds\" % (len(allmatches), endtime-starttime))\n",
    "except TimerException:\n",
    "    print(\"Excecuted %d iterations\" % count)\n",
    "finally:\n",
    "    signal.alarm(0)\n",
    "    miterations = count/1000000.0\n",
    "    pct = count/total*100.0\n",
    "    print(\"Completed %.2fM iterations (%0.2f%%)\" % (miterations, pct))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Partition Both Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done, found 462 matches in 5s\n",
      "Completed 53.85803M iterations\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "signal.alarm(timer)\n",
    "matches = []\n",
    "try:\n",
    "    starttime = time.time()\n",
    "    a_chrs = a.groupby('Chromosome')\n",
    "    b_chrs = b.groupby('chr')\n",
    "    \n",
    "    for a_chr, a_grp in a_chrs:\n",
    "        b_grp = b_chrs.get_group(a_chr)\n",
    "        for idx, a_pos in a_grp['POS'].iteritems():\n",
    "            a_match = b_grp.loc[(b_grp['start'] <= a_pos)\n",
    "                                & ((a_pos <= b_grp['stop']))]\n",
    "            if a_match.index.any():\n",
    "                matches.append(a_match)\n",
    "            count += len(b_grp.index)\n",
    "    allmatches = pd.concat(matches)\n",
    "    endtime = time.time()\n",
    "    print(\"Done, found %s matches in %ds\" % (len(allmatches.index), endtime-starttime))\n",
    "except TimerException:\n",
    "    print(\"Excecuted %d iterations\" % count)\n",
    "finally:\n",
    "    signal.alarm(0)\n",
    "    miterations = count/1000000.0\n",
    "    print(\"Completed {it}M iterations\".format(it=miterations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Review\n",
    "\n",
    "**Start with your algorithm**\n",
    "Even very efficient implementations of inefficient algorithms can be much, much slower than switching to a better, more scalable algorithm.\n",
    "\n",
    "**Optimize before multithreading**\n",
    "Modern CPUs can execute many instructions in parallel if the code and inputs let them. Use optimized libraries whenever possible (I highly recommend: **Python + Pandas + Pyspark**). Look for ways to organize your code into tight, regular loops.\n",
    "\n",
    "**Pay attention to locality**\n",
    "Caches are essential for good processor speed so try blocking your loops to cache-size chunks. **Partitioning** is one very effective technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### The Cloud\n",
    "\n",
    "What about working in **The Cloud?** Computer architecture principles still apply: pay careful attention to **latency** and **bandwidth**.\n",
    "\n",
    "**Avoid moving data**\n",
    "Communication to and from cloud-hosted servers is slow. Each message and response to and from  a cloud datacenter suffers from long latency so avoid algorithms that require lots of request and response actions. Data transfer in and out of cloud datacenters is also limited in **bandwidth**. Moving GB let alone TB of data in or out of the cloud is very slow. It's usually cheaper to send computation to a computer near the data than to send data to a far-away computer. It can also be cheaper and faster to compress files or data before transferring them.\n",
    "\n",
    "**Use Columnar Databases for *Analytics***\n",
    "There are dozens of databases to choose from that are now easily deployable or available as a service from various cloud providers. They're widely different in the way they store and process data and some are much better suited than others for a given application. For analytics and other querying, choose a columnar or vector-based database.\n",
    "\n",
    "Choose databases like:\n",
    "* Druid\n",
    "* Redshift (Amazon)\n",
    "* BigQuery (Google)\n",
    "* SQLServer (Microsoft)\n",
    "* Snowflake (on AWS)\n",
    "\n",
    "Avoid:\n",
    "* MongoDB\n",
    "* MySQL\n",
    "* Postgres\n",
    "\n",
    "There are also now several query-in-place SQL engines that have very good performance:\n",
    "* PrestoDB\n",
    "* Hive\n",
    "* Athena (Amazon hosted Presto)\n",
    "* Drill\n",
    "\n",
    "**Avoid SELECT * **\n",
    "Those columnar databases are amazingly efficient at querying, but most software that connects to them uses an interface that's row-based. That means that pulling a large result out of even a very fast database can be slow. If that database is hosted in a cloud datacenter, getting the result back can take even longer. Instead, move the computation into the database or at least to a computer in the same datacenter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Cloud Guidelines\n",
    "\n",
    "**Choose software and cloud services optimized for your use**\n",
    "Use columnar execution for computation intensive applications like data analysis. Use update-focused databases to store rapidly changing datasets.\n",
    "\n",
    "**Reduce data movement to and from The Cloud**\n",
    "Your laptop can billions of operations in the time it takes to get a response from a cloud service and your processor can pull GB/s from its local memory and SSD drive, but you'll be lucky to a few MB/s from even the best Internet services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "clinvar = pd.read_csv(CAforDS.raw.clinvar(),\n",
    "                      sep='\\t',\n",
    "                      comment='#',\n",
    "                      names=['CHROM', 'POS', 'ID', 'REF', 'ALT', 'QUAL', 'FILTER', 'INFO'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done, found 462 matches in 5s\n",
      "Completed 53.85803M iterations\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "signal.alarm(timer)\n",
    "matches = []\n",
    "try:\n",
    "    starttime = time.time()\n",
    "    a_chrs = a.groupby('Chromosome')\n",
    "    b_chrs = b.groupby('chr')\n",
    "    \n",
    "    for a_chr, a_grp in a_chrs:\n",
    "        b_grp = b_chrs.get_group(a_chr)\n",
    "        for idx, a_pos in a_grp['POS'].iteritems():\n",
    "            a_match = b_grp.loc[(b_grp['start'] <= a_pos) & ((a_pos <= b_grp['stop']))]\n",
    "            if len(a_match.index) > 0:\n",
    "                matches.append(a_match)\n",
    "            count += len(b_grp.index)\n",
    "    allmatches = pd.concat(matches)\n",
    "    endtime = time.time()\n",
    "    print(\"Done, found %s matches in %ds\" % (len(allmatches.index), endtime-starttime))\n",
    "except TimerException:\n",
    "    print(\"Excecuted %d iterations\" % count)\n",
    "finally:\n",
    "    signal.alarm(0)\n",
    "    miterations = count/1000000.0\n",
    "    print(\"Completed {it}M iterations\".format(it=miterations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    }
   ],
   "source": [
    "\n",
    "def profile_match(a, b):\n",
    "    count = 0\n",
    "    matches = []\n",
    "    a_chrs = a.groupby('Chromosome')\n",
    "    b_chrs = b.groupby('chr')\n",
    "    \n",
    "    for a_chr, a_grp in a_chrs:\n",
    "        b_grp = b_chrs.get_group(a_chr)\n",
    "        for idx, a_pos in a_grp['POS'].iteritems():\n",
    "            a_match = b_grp.loc[(b_grp['start'] <= a_pos) & ((a_pos <= b_grp['stop']))]\n",
    "            if len(a_match.index) > 0:\n",
    "                matches.append(a_match)\n",
    "            count += len(b_grp.index)\n",
    "    allmatches = pd.concat(matches)\n",
    "    return allmatches\n",
    "\n",
    "%prun -s cumulative profile_match(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dan = pd.read_csv('dan_genome/genome_Dan_Webster_v4_Full_20170504123348.txt',\n",
    "                      sep='\\t',\n",
    "                      comment='#',\n",
    "                      names=['rsid', 'chromosome', 'position', 'genotype'],\n",
    "                      dtype={'chromosome' : 'category'})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
